# -*- coding: utf-8 -*-
"""121450124_Claudhea Angeliani_TI1 M7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m5ttHHEvA3It4VLhJb4WrKluxFlXPA0b

# **PRAKTIKUM ABD MODUL 7**
**Nama : Claudhea Angeliani**\
**NIM: 121450124**\
**Kelas : RB**

## **Automated Machine Learning Pipelines**

###**Data Manipulation**
"""

pip install pyspark

from pyspark.ml.feature import StringIndexer
from pyspark.ml import Pipeline
from pyspark.sql import *
from pyspark.sql.types import *
from pyspark.sql import functions as F
from pyspark.ml.feature import IndexToString
from pyspark.sql.functions import col
from pyspark.ml.feature import StandardScaler
from pyspark.ml.feature import VectorAssembler

"""1. Missing value calculation"""

def missing_value_calculation(X, miss_per=0.75):
    missing = X.select([F.count(F.when(F.isnan(c) | F.col(c).isNull(), c)).alias(c) for c in X.columns])
    missing_len = X.count()
    final_missing = missing.toPandas().transpose()
    final_missing.reset_index(inplace=True)
    final_missing.rename(columns={0:'missing_count'}, inplace=True)
    final_missing['missing_percentage'] = final_missing['missing_count'] / missing_len
    vars_selected = final_missing['index'][final_missing['missing_percentage'] > miss_per]
    return vars_selected

"""2. Metadata Categorization"""

def identify_variable_type(X):
  l = X.dtypes
  char_vars = []
  num_vars = []
  for i in l:
    if i[1] in ('string'): char_vars.append(i[0])
    else:
      num_vars.append(i[0])
  return char_vars, num_vars

"""3. Categorical to Numerical using label encoders"""

def categorical_to_index(X, char_vars):
  chars = X.select(char_vars)
  indexers = [StringIndexer(inputCol=column, outputCol=column+"_index",handleInvalid="keep") for column in chars.columns]
  pipeline = Pipeline(stages=indexers)
  char_labels = pipeline.fit(chars)
  X = char_labels.transform(X)
  return X, char_labels

"""4. Impute Numerical Colums with a specific Value. The default is set t"""

def numerical_imputation(X,num_vars, impute_with=0):
  X = X.fillna(impute_with,subset=num_vars)
  return X

"""5. Rename categorical columns"""

def rename_columns(X, char_vars):
  mapping = dict(zip([i+ '_index' for i in char_vars], char_vars))
  X = X.select([col(c).alias(mapping.get(c, c)) for c in X.columns])
  return X

"""6. Combining features and labels"""

def join_features_and_target(X, Y):
  X = X.withColumn('id', F.monotonically_increasing_id())
  Y = Y.withColumn('id', F.monotonically_increasing_id())
  joinedDF = X.join(Y,'id','inner')
  joinedDF = joinedDF.drop('id')
  return joinedDF

"""7. Data spNitting to training, testing, and validation"""

def train_valid_test_split(df, train_size=0.4, valid_size=0.3, seed=12345):
    test_size = 1 - train_size - valid_size
    train, valid, test = df.randomSplit([train_size, valid_size, test_size], seed=seed)
    return train, valid, test

"""8. Assembling vectors"""

def assembled_vectors(train,list_of_features_to_scale,target_column_name):
  stages = []
  assembler = VectorAssembler(inputCols=list_of_features_to_scale, outputCol="features")
  stages=[assembler]
  selectedCols = [target_column_name,'features'] + list_of_features_to_scal
  pipeline = Pipeline(stages=stages)
  assembleModel = pipeline.fit(train)
  train = assembleModel.transform(train).select(selectedCols)
  return train

"""9. Scaling input variables"""

def scaled_dataframes(train, valid, test, list_of_features_to_scale, target_column_name):
    stages = []
    assembler = VectorAssembler(inputCols=list_of_features_to_scale, outputCol="assembled_features")
    scaler = StandardScaler(inputCol=assembler.getOutputCol(), outputCol="features")
    stages = [assembler, scaler]
    selected_cols = [target_column_name, 'features'] + list_of_features_to_scale
    pipeline = Pipeline(stages=stages)
    pipeline_model = pipeline.fit(train)
    train = pipeline_model.transform(train).select(selected_cols)
    valid = pipeline_model.transform(valid).select(selected_cols)
    test = pipeline_model.transform(test).select(selected_cols)
    return train, valid, test, pipeline_model

"""###**Feature Selection**"""

import pandas as pd
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt

def draw_feature_importance(user_id, mdl_ltrl, importance_df):
  importance_df = importance_df.sort_values('Importance_Score')
  plt.figure(figsize=(15,15))
  plt.title('Feature Importances')
  plt.barh(range(len(importance_df['Importance_Score'])),
importance_df['Importance_Score'], align="center")
  plt.yticks(range(len(importance_df['Importance_Score'])),
importance_df['name'])
  plt.ylabel('Variable Importance')
  plt.savefig('/home/' + user_id + '/' + 'mla_' + mdl_ltrl + '/' + 'Features seNected for modeNing.png', bbox_inches="tight")
  plt.close()
  return None

# The module below is used to save the feature importance as an Excel file
def save_feature_importance(user_id, mdl_ltrl, importance_df):
  importance_df.drop('idx',axis=1,inplace=True)
  importance_df = importance_df[0:30]
  importance_df.to_excel('/home/' + user_id + '/' + 'mla_' + mdl_ltrl + '/' + 'feature_importance.xlsx')
  draw_feature_importance(user_id, mdl_ltrl, importance_df)
  return None
# The following module is used to calculate the feature importance for each variabNe based on the Random Forest output. The feature importance is used to reduce the finaN variabNe Nist to 30.
def ExtractFeatureImp(featureImp, dataset, featuresCol):
  """
  Takes in a feature importance from a random forest / GBT model and maps it to the column names
  Output as a pandas DataFrame for easy reading
  rf = RandomForestClassifier(featuresCol="features") mod = rf.fit(train)
  ExtractFeatureImp(mod.featureImportances, train, "features")
  """
  list_extract = []
  for i in dataset.schema[featuresCol].metadata["ml_attr"]["attrs"]:
    list_extract = list_extract + dataset.schema[featuresCol].metadata["ml_attr"]["attrs"][i]
    varlist = pd.DataFrame(list_extract)
    varlist['Importance_Score'] = varlist['idx'].apply(lambda x:featureImp[x])
    return(varlist.sort_values('Importance_Score', ascending = False))

"""###**Model Building**"""

from pyspark.ml.classification import LogisticRegression
from pyspark.ml.classification import LogisticRegressionModel
# from sklearn.externals import joblib
import joblib
def logistic_model(train, x, y):
  lr = LogisticRegression(featuresCol = x, labelCol = y, maxIter = 10)
  lrModel = lr.fit(train)
  return LogisticRegressionModel
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.classification import RandomForestClassificationModel
def randomForest_model(train, x, y):
  rf = RandomForestClassifier(featuresCol = x, labelCol = y, numTrees=10)
  rfModel = rf.fit(train)
  return rfModel

from pyspark.ml.classification import GBTClassifier
from pyspark.ml.classification import GBTClassificationModel
def gradientBoosting_model(train, x, y):
  gb = GBTClassifier(featuresCol = x, labelCol = y, maxIter=10)
  gbModel = gb.fit(train)
  return gbModel
from pyspark.ml.classification import DecisionTreeClassifier
from pyspark.ml.classification import DecisionTreeClassificationModel
def decisionTree_model(train, x, y):
  dt = DecisionTreeClassifier(featuresCol = x, labelCol = y, maxDepth=5)
  dtModel = dt.fit(train)
  return dtModel
from pyspark.ml.classification import MultilayerPerceptronClassifier
from pyspark.ml.classification import MultilayerPerceptronClassificationModel
def neuralNetwork_model(train, x, y, feature_count):
  layers = [feature_count, feature_count*3, feature_count*2, 2]
  mlp = MultilayerPerceptronClassifier(featuresCol = x, labelCol = y, maxIter=100, layers=layers, blockSize=512,seed=12345)
  mlpModel = mlp.fit(train)
  return mlpModel

"""### Matrics Calculation"""

from pyspark.sql.types import DoubleType
from pyspark.sql import *
from pyspark.sql.functions import desc
from pyspark.sql.functions import udf
from pyspark.sql import functions as F
import sys
import time
# import   builtin  as builtin
import builtins
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.ml.feature import QuantileDiscretizer
import numpy
import numpy as np
from pyspark import SparkContext, HiveContext, Row, SparkConf

spark = SparkSession.builder.appName("MLA_metrics_calculator").enableHiveSupport().getOrCreate()
spark.sparkContext.setLogLevel('ERROR')
sc = spark.sparkContext

def highlight_max(data, color="yellow"):
    '''
    highlight the maximum in a Series or DataFrame
    '''
    attr = 'background-color: {}'.format(color)
    if data.ndim == 1: # Series from .apply(axis=0) or axis=1 is_max = data == data.max()
        return [attr if v else '' for v in is_max]
    else: # from .apply(axis=None)
        is_max = data == data.max().max()
        return pd.DataFrame(np.where(is_max, attr, ''),index=data.index, columns=data.columns)

def calculate_metrics(predictions, y, data_type):
    start_time = time.time()
    # Calculate ROC
    evaluator = BinaryClassificationEvaluator(labelCol=y, rawPredictionCol='prediction')
    auroc = evaluator.evaluate(predictions, {evaluator.metricName: "areaUnderROC"})
    print('AUC calculated:', auroc)

    selectedCols = predictions.select(F.col("probability"), F.col('prediction'), F.col(y))
    y_score, y_pred, y_true = zip(*selectedCols)

    # Calculate Accuracy
    accuracy_df = predictions.withColumn('acc',F.when(predictions.prediction==predictions[y],1).otherwise(0))
    accuracy_df.createOrReplaceTempView("accuracyTable")
    RF_accuracy = spark.sql("select sum(acc)/count(1) as accuracy from accuracyTable")
    print('Accuracy calculated:', RF_accuracy)

    # Build KS Table
    split1_udf = udf(lambda value: value[1].item(), DoubleType())
    if data_type in ['train','valid','test','oot1','oot2']:
        decileDF = predictions.select(y, split1_udf('probability').alias('probs'))
    else:
        decileDF = predictions.select(y, 'probability')
    decileDF = decileDF.withColumn('non_target',1-decileDF[y])
    window = Window.orderBy(desc("probability"))
    decileDF = decileDF.withColumn("rownum", F.row_number().over(window))
    decileDF.cache()
    decileDF = decileDF.withColumn("rownum",decileDF["rownum"].cast("double"))
    window2 = Window.orderBy("rownum")
    RF_bucketedData = decileDF.withColumn("deciles", F.ntile(10).over(window2))
    RF_bucketedData = RF_bucketedData.withColumn('deciles',RF_bucketedData['deciles']).cache()
    print('KS Calculation starting')
    target_cnt = RF_bucketedData.groupBy('deciles').agg(F.sum(y).alias('target'))
    non_target_cnt = RF_bucketedData.groupBy('deciles').agg(F.sum("non_target").alias('non_target'))
    overall_cnt = RF_bucketedData.groupBy('deciles').count().alias('Total').toPandas()
    overall_cnt = overall_cnt.merge(target_cnt,on='deciles',how='inner').merge(non_target_cnt,on='deciles',how='inner')
    overall_cnt = overall_cnt.sort_values(by='deciles',ascending=True)
    overall_cnt['Pct_target']=(overall_cnt['target']/overall_cnt['count'])*10
    overall_cnt['cum_target'] = overall_cnt.target.cumsum()
    overall_cnt['cum_non_target'] = overall_cnt.non_target.cumsum()
    overall_cnt['%Dist_Target'] = (overall_cnt['cum_target'] / overall_cnt.target.sum())*10
    overall_cnt['%Dist_non_Target'] = (overall_cnt['cum_non_target'] / overall_cnt.non_target.sum())*10
    overall_cnt['spread'] = builtins.abs(overall_cnt['%Dist_Target']-overall_cnt['%Dist_non_Target'])
    decile_table = overall_cnt.round(2)
    print("KS_Value =", builtins.round(overall_cnt.spread.max(),2))
    decileDF.unpersist()
    RF_bucketedData.unpersist()
    print("Metrics Calculation process Completed in : "+ " %s seconds" % (time.time() - start_time))
    return auroc, RF_accuracy, builtins.round(overall_cnt.spread.max(),2), y_score

"""### **Validation and Plot Generation**"""

import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from sklearn import metrics
import glob
import os
import pandas as pd
import seaborn as sns
from pandas import ExcelWriter

# Function to highlight maximum value
def highlight_max(data, color="yellow"):
    attr = 'background-color: {}'.format(color)
    if data.ndim == 1:
        is_max = data == data.max()
        return [attr if v else '' for v in is_max]
    else:
        is_max = data == data.max().max()
        return pd.DataFrame(np.where(is_max, attr, ''),index=data.index, columns=data.columns)

# Function to draw ROC plot
def draw_roc_plot(user_id, mdl_name, y_score, y_true, model_type, data_type):
    fpr, tpr, thresholds = metrics.roc_curve(y_true, y_score, pos_label=1)
    roc_auc = metrics.auc(fpr, tpr)
    plt.title(str(model_type) + ' Model - ROC for ' + str(data_type) + ' data')
    plt.plot([0, 1], [0, 1], 'r--')
    plt.plot(fpr, tpr, label = 'AUC = %0.2f' % roc_auc)
    plt.xlabel('False Positive Rate (1 - Specificity)')
    plt.ylabel('True Positive Rate (Sensitivity)')
    plt.legend(loc = 'lower right')
    plt.savefig('/home/' + user_id + '/' + 'mla_' + mdl_name + '/' + str(model_type) + '/' + str(model_type) + ' Model - ROC for ' + str(data_type) + ' data.png', bbox_inches="tight")
    plt.close()

# Function to draw KS plot
def draw_ks_plot(user_id, mdl_name, model_type):
    writer = ExcelWriter('/home/' + user_id + '/' + 'mla_' + mdl_name + '/' + str(model_type) + '/KS_Charts.xlsx')
    for filename in glob.glob('/home/' + user_id + '/' + 'mla_' + mdl_name + '/' + str(model_type) + '/KS ' + str(model_type) + ' Model*.xlsx'):
        excel_file = pd.ExcelFile(filename)
        (f, f_name) = os.path.split(filename)
        (f_short_name, _) = os.path.splitext(f_name)
        for sheet_name in excel_file.sheet_names:
            df_excel = pd.read_excel(filename, sheet_name=sheet_name)
            df_excel = df_excel.style.apply(highlight_max, subset=['spread'], color='#e6b71e')
            df_excel.to_excel(writer, f_short_name, index=False)
            worksheet = writer.sheets[f_short_name]
            worksheet.conditional_format('C2:C11', {'type': 'data_bar', 'bar_color': '#34b5d9'})
            worksheet.conditional_format('E2:E11', {'type': 'data_bar', 'bar_color': '#366fff'})
        os.remove(filename)
    writer.save()

# Function to draw confusion matrix
def draw_confusion_matrix(user_id, mdl_name, y_pred, y_true, model_type, data_type):
    AccuracyValue = metrics.accuracy_score(y_pred=y_pred, y_true=y_true)
    PrecisionValue = metrics.precision_score(y_pred=y_pred, y_true=y_true)
    RecallValue = metrics.recall_score(y_pred=y_pred, y_true=y_true)
    F1Value = metrics.f1_score(y_pred=y_pred, y_true=y_true)
    plt.title(str(model_type) + ' Model - Confusion Matrix for ' + str(data_type) + ' data \n \n Accuracy:{0:.3f} Precision:{1:.3f} Recall: {2:.3f} F1 Score:{3:.3f}\n'.format(AccuracyValue, PrecisionValue, RecallValue, F1Value))
    cm = metrics.confusion_matrix(y_true=y_true, y_pred=y_pred)
    sns.heatmap(cm, annot=True, fmt="g")
    plt.xlabel("Predicted Labels")
    plt.ylabel("True Labels")
    plt.savefig('/home/' + user_id + '/' + 'mla_' + mdl_name + '/' + str(model_type) + '/' + str(model_type) + ' Model - Confusion Matrix for ' + str(data_type) + ' data.png', bbox_inches="tight")
    plt.close()

# Model validation function
def model_validation(user_id, mdl_name, data, y, model, model_type, data_type):
    start_time = time.time()
    pred_data = model.transform(data)
    print('Model output predicted')
    roc_data, accuracy_data, ks_data, y_score, y_pred, y_true, decile_table = calculate_metrics(pred_data, y, data_type)
    draw_roc_plot(user_id, mdl_name, y_score, y_true, model_type, data_type)
    decile_table.to_excel('/home/' + user_id + '/' + 'mla_' + mdl_name + '/' + str(model_type) + '/KS ' + str(model_type) + ' Model ' + str(data_type) + '.xlsx', index=False)
    draw_confusion_matrix(user_id, mdl_name, y_pred, y_true, model_type, data_type)
    print('Metrics computed')
    l = [roc_data, accuracy_data, ks_data]
    end_time = time.time()
    print("Model validation process completed in : %s seconds" % (end_time- start_time))
    return l

"""### **Model Selection**"""

import pandas as pd
import joblib
import numpy as np
import glob
import os

def select_model(user_id, mdl_name, model_selection_criteria, dataset_to_use):
    df = pd.DataFrame({}, columns=['roc_train', 'accuracy_train', 'ks_train', 'roc_valid', 'accuracy_valid', 'ks_valid', 'roc_test', 'accuracy_test', 'ks_test', 'roc_oot1', 'accuracy_oot1', 'ks_oot1', 'roc_oot2', 'accuracy_oot2', 'ks_oot2'])
    current_dir = os.getcwd()
    os.chdir('/home/' + user_id + '/' + 'mla_' + mdl_name)
    for file in glob.glob('*metrics.pkl'):
        loaded_data = joblib.load(file)
        df.loc[str(file.split('_')[0])] = loaded_data

    for file in glob.glob('*metrics.pkl'):
        os.remove(file)
    os.chdir(current_dir)
    df.index = df.index.set_names(['model_type'])
    df = df.reset_index()
    model_selection_criteria = model_selection_criteria.lower()
    column_to_sort = model_selection_criteria + '_' + dataset_to_use.lower()
    threshold_value = 0.03
    if model_selection_criteria == 'ks':
        threshold_value = threshold_value * 100
    df['counter'] = (
        (np.abs(df[column_to_sort] - df[model_selection_criteria + '_train']) > threshold_value).astype(int) +
        (np.abs(df[column_to_sort] - df[model_selection_criteria + '_valid']) > threshold_value).astype(int) +
        (np.abs(df[column_to_sort] - df[model_selection_criteria + '_test']) > threshold_value).astype(int) +
        (np.abs(df[column_to_sort] - df[model_selection_criteria + '_oot1']) > threshold_value).astype(int) +
        (np.abs(df[column_to_sort] - df[model_selection_criteria + '_oot2']) > threshold_value).astype(int)
    )

    df = df.sort_values(by=['counter', column_to_sort], ascending=[True, False]).reset_index(drop=True)
    df['selected_model'] = ''
    df.loc[0, 'selected_model'] = 'Champion'
    df.loc[1, 'selected_model'] = 'Challenger'
    df.to_excel('/home/' + user_id + '/' + 'mla_' + mdl_name + '/metrics.xlsx')
    return df

"""### **Score Code Creation**"""

import string

import_packages = """
# This is a pseudo score code for production deployment. It links to all your created during the modeling process. If you plan to use this file, then change "score_table" variable to point to your input data. Double-check the "home_path" and "hdfs_path" if you altered the location of model objects.
import os
os.chdir('/home/jovyan/work/spark-warehouse/auto_model_builder')
from pyspark import SparkContext, HiveContext, Row, SparkConf
from pyspark.sql import *
from pyspark.ml import Pipeline
from pyspark.ml.linalg import Vectors, VectorUDT
from pyspark.sql.functions import *
from pyspark.mllib.stat import *
from pyspark.ml.feature import *
from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer
from sklearn.metrics import roc_curve, auc
import numpy as np
import pandas as pd
import subprocess
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator
from pyspark.ml import Pipeline, PipelineModel
from pyspark.sql import functions as func
from datetime import *
from pyspark.sql import SparkSession, SQLContext
from pyspark.sql.types import *
from dateutil.parser import parse
from data_manipulations import *
from model_builder import *
import datetime
import string
import os
import sys
import time
import numpy

spark = SparkSession.builder.appName("MLA_Automated_Scorecode").enableHiveSupport().getOrCreate()
spark.sparkContext.setLogLevel('ERROR')
sc = spark.sparkContext
"""

parameters = string.Template("""
user_id = '${user_id}'
model_output_id = '${model_output_id}'
model_train = '${model_train}'
# Since the hdfs and home path below are pointing to your user_id by default, for scoring, you need to upload the model objects in hdfs_path and home_path appropriate score location path (Could be advanced or any other folder path). Y the following files to perform scoring.
# hdfs_path - All the files in the path specified below
# home_path - 'model_scoring_info.z'
hdfs_path = '/user/${user_id}' + '/' + 'model_${model_train}' # update score location
home_path = '/home/${user_id}' + '/' + 'model_${model_train}' # update score location
""")

import_variables = """
from sklearn.externals import joblib
from pyspark.ml import Pipeline, PipelineModel
final_vars, id_vars, vars_selected, char_vars, num_vars, impute_with, selected_model = joblib.load(home_path + '/model_scoring_info.z')
char_names = PipelineModel.load(hdfs_path + '/char_names_model.h5')
pipelineModel = PipelineModel.load(hdfs_path + '/pipelineModel.h5')
"""

load_models = """
KerasModel = ''
loader_model_list = [LogisticRegressionModel, RandomForestClassification, GBTClassificationModel, DecisionTreeClassificationModel, MultilayerPerceptronClassificationModel, KerasModel]
models_to_run = ['logistic', 'randomForest', 'gradientBoosting', 'decisionTree', 'neuralNetwork', 'keras']

for model in models_to_run:
    try:
        load_model = loader_model_list[models_to_run.index(model)]
        model = load_model.load(hdfs_path + '/' + model + '_model.h5')
    except:
        pass
"""

score_function = """
score_table = spark.sql("select " + ", ".join(final_vars) + " from " + dev_tab) # update this query appropriately

def score_new_df(score_df, model):
    new_X = score_df.select(final_vars)
    new_X = new_X.select(vars_selected)
    new_X = char_names.transform(new_X)
    new_X = numeric_imputation(new_X, num_vars, impute_with)
    new_X = new_X.select([c for c in new_X.columns if c not in char_vars])
    new_X = rename_columns(new_X, char_vars)
    final_score_DF = pipelineModel.transform(new_X)
    final_score_DF.cache()
    final_predicted_DF = model.transform(final_score_DF)
    final_predicted_DF.cache()
    return final_predicted_DF

ScoredDF = score_new_df(score_table, selected_model)
"""

def selected_model_scorecode(user_id, model_output_id, model_ltrl, parameters):
    parameters = parameters.substitute(locals())
    scorefile = open('/home/' + user_id + '/' + 'mla_' + model_ltrl + '/score_code_selected_model.py', 'w')
    scorefile.write(import_packages)
    scorefile.write(parameters)
    scorefile.write(import_variables)
    scorefile.write(load_models)
    scorefile.write(score_function)
    scorefile.close()
    print('Score code generation complete')

def individual_model_scorecode(user_id, model_output_id, model_ltrl, parameters):
    loader_model_list = [LogisticRegressionModel, RandomForestClassification, GBTClassificationModel,
                         DecisionTreeClassificationModel, MultilayerPerceptronClassificationModel, KerasModel]
    models_to_run = ['logistic', 'randomForest', 'gradientBoosting', 'decisionTree', 'neuralNetwork', 'keras']
    parameters = parameters.substitute(locals())
    for model in models_to_run:
        try:
            load_model = loader_model_list[models_to_run.index(model)]
            write_model_parameter = string.Template("""model = ${load_model}.load(hdfs_path + '/' + ${model} + '_model.h5') """).substitute(locals())
            scorefile = open('/home/' + user_id + '/' + 'mla_' + model_ltrl + '/' + model[0].upper() + model[1:] + '/score_code_' + model + '_model.py', 'w')
            scorefile.write(import_packages)
            scorefile.write(parameters)
            scorefile.write(import_variables)
            scorefile.write(write_model_parameter)
            scorefile.write(score_function)
            scorefile.close()
        except:
            pass
    print('Individual Score code generation complete')

"""### **Collating Result**"""

import os
import zipfile

def retrieve_file_paths(dir_name):
    file_paths = []
    # Read all directory, subdirectories, and files
    for root, directories, files in os.walk(dir_name):
        for filename in files:
            # Create the full file path using os module
            file_path = os.path.join(root, filename)
            file_paths.append(file_path)
    # return all paths
    return file_paths

# Define the main function
def zipper(dir_name):
    # Call the function to retrieve all files and folders of the assigned directory
    file_paths = retrieve_file_paths(dir_name)
    # printing the list of all files to be zipped
    print('The following list of files will be zipped:')
    for file_name in file_paths:
        print(file_name)
    # writing files to a zipfile
    zip_file = zipfile.ZipFile(dir_name+'.zip', 'w')
    with zip_file:
        for file in file_paths:
            zip_file.write(file)
    print(dir_name+'.zip file is created successfully!')
    return dir_name+'.zip'

"""### **Framework**"""

from pyspark import SparkContext, HiveContext, Row, SparkConf
from pyspark.sql import *
from pyspark.ml import Pipeline
from pyspark.ml.linalg import Vectors, VectorUDT
from pyspark.sql.functions import *
from pyspark.ml.stat import *
from pyspark.ml.feature import *
from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer
from sklearn.metrics import roc_curve, auc
import numpy as np
import pandas as pd
import subprocess

from pyspark.ml.tuning import ParamGridBuilder, CrossValidator
from pyspark.ml import Pipeline, PipelineModel
from pyspark.sql import functions as func
from datetime import *
from pyspark.sql import SparkSession, SQLContext
from pyspark.sql.types import *
from datetime import date
import string
import os
import sys
import time
import numpy

spark = SparkSession.builder.appName("Automated_model_building").enableHiveSupport().getOrCreate()
spark.sparkContext.setLogLevel('ERROR')
sc = spark.sparkContext

import_data = False
stop_run = False
message = ''
filename = ''
user_id = 'jovyan'
model_output_id = 'test_run01'
model_tag = 'chapter8_testrun'
input_dev_file = 'churn_modeling.csv'
input_oot1_file = ''
input_oot2_file = ''
dev_table_name = ''
oot1_table_name = ''
oot2_table_name = ''
delimiter_type = ','
include_vars = ''
include_prefix = ''
include_suffix = ''
exclude_vars = 'rownumber, customerid, surname'
exclude_prefix = ''
exclude_suffix = ''
target_column_name = 'exited'
run_logistic_model = 1
run_randomforest_model = 1
run_boosting_model = 1
run_neural_model = 1
miss_per = 0.75
impute_with = 0.0
train_size = 0.7
valid_size = 0.2
seed = 2308
model_selection_criteria = 'ks'
dataset_to_use = 'train'
data_folder_path = '/home/jovyan/work/'
hdfs_folder_path = '/home/jovyan/work/spark-warehouse/'

####################################################################
###### No input changes required below this for default run##########
####################################################################

if input_oot1_file == '':
    input_oot1_file = input_dev_file

if input_oot2_file == '':
    input_oot2_file = input_dev_file

# assign input files if the user uploaded files instead of tables.
if dev_table_name.strip() == '':
    dev_input_file = input_dev_file
    if dev_input_file.strip() == '':
        print('Please provide a development table or development file to process.')
        stop_run = True
        message = 'Development Table or file is not provided. Please provide'
    else:
        fi_type = dev_table_name.split('.')[-1]
        out, err = subprocess.Popen(['cp', data_folder_path + dev_input_file, hdfs_folder_path + dev_input_file]).communicate()
if oot1_table_name.strip() == '':
    oot1_input_file = input_oot1_file
    out, err = subprocess.Popen(['cp', data_folder_path + oot1_input_file, hdfs_folder_path + oot1_input_file]).communicate()

if oot2_table_name.strip() == '':
    oot2_input_file = input_oot2_file
    out, err = subprocess.Popen(['cp', data_folder_path + oot2_input_file, hdfs_folder_path + oot2_input_file]).communicate()

ignore_data_type = ['timestamp', 'date']
ignore_vars_based_on_datatype = []

# extract the input variables in the file or table if not stop_run:
if import_data:
    df = spark.read.option("delimiter", delimiter_type).option("header", "true").csv(dev_input_file)
    df = pd.DataFrame(zip(*df.dtypes),['column_name', 'data_type']).T
else:
    df = spark.sql('describe ' + dev_table_name)
    df = df.toPandas()

input_vars = list(str(x[0]) for x in df['column_name'])
print(input_vars)
for i in ignore_data_type:
    ignore_vars_based_on_datatype += list(str(x) for x in df[df['data_type'] == i]['column_name'])
if len(ignore_vars_based_on_datatype) > 0:
    input_vars = list(set(input_vars) - set(ignore_vars_based_on_datatype))
input_vars.remove(target_column_name)

import re
from pyspark.sql.functions import col
# import data for the modeling
if import_data:
    train_table = spark.read.option("delimiter", delimiter_type).option("header", header_type).csv(dev_input_file)
    oot1_table = spark.read.option("delimiter", delimiter_type).option("header", header_type).csv(oot1_input_file)
    oot2_table = spark.read.option("delimiter", delimiter_type).option("header", header_type).csv(oot2_input_file)
else:
    train_table = spark.sql("select " + ", ".join(final_vars + [target_column_name]) + " from " + dev_table_name)
    oot1_table = spark.sql("select " + ", ".join(final_vars + [target_column_name]) + " from " + oot1_table_name)
    oot2_table = spark.sql("select " + ", ".join(final_vars + [target_column_name]) + " from " + oot2_table_name)

train_table = train_table.where(col(target_column_name).isNotNull())
oot1_table = oot1_table.where(col(target_column_name).isNotNull())
oot2_table = oot2_table.where(col(target_column_name).isNotNull())

print(final_vars)
oot1_table = oot1_table.toDF(*[c.lower() for c in oot1_table.columns])
oot2_table = oot2_table.toDF(*[c.lower() for c in oot2_table.columns])
print(oot1_table.columns)
print(oot2_table.columns)

X_train = train_table.select(*final_vars)
X_train.cache()

# apply data manipulations on the data - missing value check, name encoding
from data_manipulations import *
vars_selected_train = missing_value_calculation(X_train, miss_per)
vars_selected = filter(None, list(set(vars_selected_train)))
print('vars selected')
X = X_train.select(*vars_selected)
print(X.columns)
vars_selectedn=X.columns
X = X.cache()
Y = train_table.select(target_column_name)
Y = Y.cache()
char_vars, num_vars = identify_variable_type(X)
X, char_names = categorical_to_index(X, char_vars) #Name encoding
X = numeric_imputation(X, num_vars, impute_with) # imputation
X = X.select([c for c in X.columns if c not in char_vars])

X = rename_columns(X, char_vars)
joinedDF = join_features_and_target(X, Y)
joinedDF = joinedDF.cache()
print('Features and targets are joined')
train, valid, test = train_valid_test_split(joinedDF, train_size, valid_s train = train.cache())
valid = valid.cache()
test = test.cache()
print('Train, valid and test dataset created')
x = train.columns
x.remove(target_column_name)
feature_count = len(x)
print(feature_count)
if feature_count > 30:
    print('# No of features - ' + str(feature_count) + '., Performing features')
# directory to produce the outputs of the automation
import os
try:
    if not os.path.exists('/home/' + user_id + '/' + 'mla_' + mdl_ltrl):
        os.mkdir('/home/' + user_id + '/' + 'mla_' + mdl_ltrl)
except:
    user_id = 'jovyan'
    if not os.path.exists('/home/' + user_id + '/' + 'mla_' + mdl_ltrl):
        os.mkdir('/home/' + user_id + '/' + 'mla_' + mdl_ltrl)
subprocess.call(['chmod','777','-R','/home/' + user_id + '/' + 'mla_' + md x = train.columns])
x.remove(target_column_name)
sen_train = assembled_vectors(train, x, target_column_name)
sen_train.cache()

# # Variable Reduction for more than 30 variables in the feature set using
from pyspark.ml.classification import RandomForestClassifier
from feature_selection import *
rf = RandomForestClassifier(featuresCol="features", labelCol=target_column mod = rf.fit(sen_train))
varlist = ExtractFeatureImp(mod.featureImportances, sen_train, "features") selected_vars = [str(x) for x in varlist['name'][0:30]]
train = train.select([target_column_name] + selected_vars)
train.cache()
save_feature_importance(user_id, mdl_ltrl, varlist) #Create feature impo x = train.columns
x.remove(target_column_name)
feature_count = len(x)
print(feature_count)
train, valid, test, pipelineModel = scaled_dataframes(train, valid, tes)
train = train.cache()
valid = valid.cache()
test = test.cache()
print('Train, valid and test are scaled')
print(train.columns)
# import packages to perform model building, validation, and plots
import time
from validation_and_plots import *
# apply the transformation done on training dataset to OOT 1 and OOT 2 us def score_new_df(scoredf):
    newX = scoredf.select(*final_vars) #idX = scoredf.select(id_vars) print(newX.columns)
    newX = newX.select(*vars_selectedn) print(newX.columns)
    newX = char_names.transform(newX)
    newX = numeric_imputation(newX, num_vars, impute_with)
    newX = newX.select([c for c in newX.columns if c not in char_vars]) newX = rename_columns(newX, char_vars)
    final_score_DF = pipelineModel.transform(newX)
    final_score_DF.cache()
    return final_score_DF
# apply the transformation done on training dataset to OOT 1 and OOT 2 us x = 'features'
y = target_column_name
oot1_targetY = oot1_table.select(target_column_name) print(oot1_table.columns)
oot1_intDF = score_new_df(oot1_table)
oot1_finalDF = join_features_and_target(oot1_intDF, oot1_targetY) oot1_finalDF.cache()
print(oot1_finalDF.dtypes)
oot2_targetY = oot2_table.select(target_column_name) oot2_intDF = score_new_df(oot2_table)
oot2_finalDF = join_features_and_target(oot2_intDF, oot2_targetY) oot2_finalDF.cache()
print(oot2_finalDF.dtypes) # run individual models
from model_builder import *
from metrics_calculator import *
loader_model_list = []
dataset_list = ['train', 'valid', 'test', 'oot1', 'oot2']
datasets = [train, valid, test, oot1_finalDF, oot2_finalDF]
print(train.count())
print(test.count())
print(valid.count())
print(oot1_finalDF.count())
print(oot2_finalDF.count())
models_to_run = []
if run_logistic_model:
    lrModel = logistic_model(train, x, y) #build model lrModel.write().overwrite().save('/home/' + user_id + '/' + 'mla_' + print("Logistic model developed")
    model_type = 'Logistic'
    N = []
    try:
        os.mkdir('/home/' + user_id + '/' + 'mla_' + mdl_ltrl + '/' + str) except:
        pass
    for i in datasets :
        N += model_validation(user_id, mdl_ltrl, i, y, lrModel, model_type)
    draw_ks_plots(user_id, mdl_ltrl, model_type) #ks charts joblib.dump(N,'/home/' + user_id + '/' + 'mla_' + mdl_ltrl + '/logistic_mo models_to_run.append('logistic')
    loader_model_list.append(LogisticRegressionModel)
if run_randomforest_model:
    rfModel = randomForest_model(train, x, y) #build model rfModel.write().overwrite().save('/home/' + user_id + '/' + 'mla_' + print("Random Forest model developed")
    model_type = 'RandomForest'
    N = []
    try:
        os.mkdir('/home/' + user_id + '/' + 'mla_' + mdl_ltrl + '/' + str except:
        pass)
    for i in datasets:
        N += model_validation(user_id, mdl_ltrl, i, y, rfModel, model_type)
    draw_ks_plots(user_id, mdl_ltrl, model_type) #ks charts joblib.dump(N,'/home/' + user_id + '/' + 'mla_' + mdl_ltrl + '/randomfo models_to_run.append('randomForest')
    loader_model_list.append(RandomForestClassificationModel)
if run_gradientboosting_model:
    gbModel = gradientBoosting_model(train, x, y) #build model gbModel.write().overwrite().save('/home/' + user_id + '/' + 'mla_' + print("Gradient Boosting model developed")
    model_type = 'GradientBoosting'
    N = []
    try:
        os.mkdir('/home/' + user_id + '/' + 'mla_' + mdl_ltrl + '/' + str except:
        pass)
    for i in datasets :
        N += model_validation(user_id, mdl_ltrl, i, y, gbModel, model_type)
    draw_ks_plots(user_id, mdl_ltrl, model_type) #ks charts joblib.dump(N,'/home/' + user_id + '/' + 'mla_' + mdl_ltrl + '/gradient models_to_run.append('gradientBoosting')
    loader_model_list.append(GBTClassificationModel)
if run_neuralnetwork_model:
    mlpModel = neuralNetwork_model(train, x, y, feature_count) #build mod mlpModel.write().overwrite().save('/home/' + user_id + '/' + 'mla_' + print("Neural Network model developed")
    model_type = 'NeuralNetwork'
    N = []
    try:
        os.mkdir('/home/' + user_id + '/' + 'mla_' + mdl_ltrl + '/' + str except:
        pass)
    for i in datasets:
        N += model_validation(user_id, mdl_ltrl, i, y, mlpModel, model_type)
    draw_ks_plots(user_id, mdl_ltrl, model_type) #ks charts joblib.dump(N,'/home/' + user_id + '/' + 'mla_' + mdl_ltrl + '/neuralne models_to_run.append('neuralNetwork')
    loader_model_list.append(MultilayerPerceptronClassificationModel)
# model building complete. Let us validate the metrics for the models cre # model validation part starts now.
from model_selection import *
output_results = select_model(user_id, mdl_ltrl, model_selection_criteria provided by user)
#print(type(output_results), output_results)
selected_model = output_results['model_type'][0] #Champion model based on
Lead_model = Loader_model_list[models_to_run.index(selected_model)] #Load model = Load_model.Load('/home/' + user_id + '/' + 'mla_' + mdl_ltrl + '/'
print('Model selected for scoring - ' + selected_model) # Produce pseudo score for production deployment
# save objects produced in the steps above for future scoring
import joblib
char_names.write().overwrite().save('/home/' + user_id + '/' + 'mla_' + pipelineModel.write().overwrite().save('/home/' + user_id + '/' + 'mla_'))
save_list = [final_vars, vars_selected, char_vars, num_vars, impute_with, scen joblib.dump](save_list, '/home/' + user_id + '/' + 'mla_' + mdl_ltrl + '/mo')
# # Create score code
from scorecode_creator import *
selected_model_scorecode(user_id, mdl_output_id, mdl_ltrl, parameters) individual_model_scorecode(user_id, mdl_output_id, mdl_ltrl, parameters)
message = message + 'Model building activity complete and the results are'
from zipper_function import * try:
    filename = zipper('/home/' + user_id + '/' + 'mla_' + mdl_ltrl) except:
    filename = ''
# Clean up files Loaded in the Local path
if import_data:
    file_list = [dev_input_file, oot1_input_file, oot2_input_file]
    for i in list(set(file_list)):
        try:
            os.remove(data_folder_path + str(i)) except:
            pass
# Clean up files Loaded in the hdfs path
file_list = [dev_input_file, oot1_input_file, oot2_input_file]
for i in list(set(file_list)):
    try:
        out, err = subprocess.Popen(['rm', '-r', '-f'], hdfs_folder_path+str(i) except:
        pass)