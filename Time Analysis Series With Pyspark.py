# -*- coding: utf-8 -*-
"""121450124_Claudhea Angeliani_TI1 M8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aqcuoowPmYGCBAfzqfjsczjRjgCTwlRp

# Modul 8 Analisis Big Data
# Time Series Analysis dengan PySpark

#Import useful libraries
"""

pip install pyspark

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.session import SparkSession
from pyspark.sql import *
from pyspark.sql.types import *
from pyspark.sql.functions import *
from pyspark import SparkContext, SparkConf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime

"""#Read dataset from csv

**NB**: For some reason yahoo finance responded with 403 when trying to download the dataset directly with wget, so the file "BTC_USD.csv" (included in the archive) needs to be added to DBFS at "dbfs:/FileStore/BTC_USD/BTC_USD.csv"
"""

#from pyspark.sql import SparkSession

# Create a SparkSession
spark = SparkSession.builder \
    .appName("Read BTC-USD data") \
    .getOrCreate()

# Read the CSV file into a DataFrame
df = spark.read \
    .format("csv") \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .load("/content/BTC-USD.csv")

# Show the DataFrame schema
df.printSchema()

# Show the first few rows of the DataFrame
df.show()

"""Diatas adalah kode untuk mnegimport dataset untuk diolah. PErtama, harus dibangun sesi spark terlebih dahulu, baru membaca data. Data yang terbaca dihasilkan dalam bentuk skema roor dan dataframe time series.

#Train-test split

Validation not needed because CrossValidator will use part of train set as validation set (KFold)

Proportion of split is ~ 70/30
"""

# Filter the DataFrame for train and test sets
train_set = df.filter(col("Date") < "2019-09-17")
test_set = df.filter(col("Date") >= "2019-09-17")

# Show the number of rows in each set
print("Number of rows in train set:", train_set.count())
print("Number of rows in test set:", test_set.count())

"""Langkah ini digunakan untuk membagi dataset kedalam data train dan data test dengan proporsi 70/30. Hasilnya, untuk train set sebanyak 1826 baris dan data test sebanayak 861 baris.

#Feature importance analysis

Pearson and Spearman correlation matrices used to study the correlation between each couple of features.

Being all features quite highly correlated, I choose to keep just one of them (close price) to be able to use a window of more days in the models
"""

from pyspark.ml.stat import Correlation
from pyspark.ml.feature import VectorAssembler
import numpy as np

def show_matrix(matrix):
    """
    Function to print a matrix on screen
    """
    correlation_array = matrix.collect()[0]["pearson({})".format(vector_col)].toArray()
    print(correlation_array)

vector_col = "features"
assembler = VectorAssembler(inputCols=["Open", "Close", "High", "Low", "Volume"], outputCol=vector_col)
df_vector = assembler.transform(df).select(vector_col)

matrix_pearson = Correlation.corr(df_vector, vector_col)  # Pearson is the default

print("Pearson correlation matrix:")
show_matrix(matrix_pearson)

"""Kode tersebut digunakan untuk menghitung matriks korelasi antara kolom-kolom numerik dalam DataFrame menggunakan korelasi Pearson. Ini dapat membantu dalam menganalisis hubungan antara variabel-variabel tersebut. Kode ini membantu dalam mengevaluasi korelasi antara variabel numerik dalam DataFrame.

#Feature scaling

Tanh estimator used to scale all the feature values
"""

import math

from pyspark.sql.functions import mean as _mean, stddev as _stddev, col, udf
from pyspark.sql.types import FloatType, StructType, ArrayType


train_set_stats = train_set.select(
    _mean(col('Close')).alias('mean'),
    _stddev(col('Close')).alias('std')
).collect()
mean = train_set_stats[0]['mean']    #mean of close prices
std = train_set_stats[0]['std']    #standard deviation of close prices


@udf(returnType=FloatType())
def tanh_estimator(x):
    """
    user defined function, applies tanh estimator's formula to a feature value x
    """
    return (float)(0.5 * (np.tanh(0.01*(x-mean)/std) + 1))

def scale_transform(df):
    """
    tranforms a dataframe applying tanh estimator udf
    """
    return df.select("Date", tanh_estimator("Close").alias("Close"))

scaled_train_set = scale_transform(train_set)
scaled_test_set = scale_transform(test_set)

scaled_train_set.show()
scaled_test_set.show()

"""Kode yang diberikan bertujuan untuk melakukan transformasi data time series kedalam bentuk dataframe. Pertama-tama, modul dan fungsi yang diperlukan dari PySpark diimpor untuk memfasilitasi operasi-operasi yang akan dilakukan. Setelah itu, dilakukan penghitungan rata-rata dan deviasi standar dari kolom 'Close' dalam DataFrame `train_set`, yang kemudian digunakan dalam fungsi pengubah pengguna yang didefinisikan pengguna (`tanh_estimator`). Fungsi tersebut menerapkan rumus tangen hiperbolik (tanh) ke setiap nilai fitur, menggunakan nilai rata-rata dan deviasi standar yang dihitung sebelumnya. Fungsi ini kemudian diaplikasikan ke DataFrame menggunakan fungsi `scale_transform`, yang menghasilkan DataFrame baru (`scaled_train_set` dan `scaled_test_set`) dengan kolom 'Close' yang telah diubah sesuai dengan pendekatan tersebut. DataFrame yang telah diubah kemudian ditampilkan untuk pemeriksaan lebih lanjut. Dengan demikian, kode tersebut memberikan kerangka kerja untuk mentransformasi data secara khusus dengan menggunakan metode yang telah ditentukan.

#Sliding window

Window of 30 days is slided on the close prices in order to create train and test set, composed by examples such as:

x={day(i), ... , day(i+29)}, y={day(i+30)}
"""

from pyspark.sql.window import Window

def slide_window(df, window_size):
    """
    Returns two new dataframes:
    X - obtained sliding a window of given size (=#window_size) on the original dataframe, aggregating #window_size close prices on the same row
    y - for each row of X, y contains a row with the (single) price of the day after last day contained in X
    """

    w = Window.orderBy("Date")
    indexed_df = df.withColumn("Index", row_number().over(w)).select("Index", "Close")    #adding index to be able to loop following order and create windows

    schema = StructType([StructField("Close", ArrayType(FloatType()), False)])   #schema for X (array of floats)

    X = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema )
    y = spark.createDataFrame(spark.sparkContext.emptyRDD(), FloatType())

    length = indexed_df.count()
    for i in range(window_size+1, length+1):
        new_df = indexed_df.where(col("Index").between(i-window_size, i-1)).select("Close")    #select the window
        new_row = new_df.agg(collect_list("Close").alias("Close"))    #create new X's row with all prices from window
        X = X.union(new_row)
        new_row = indexed_df.where(col("Index") == i).select("Close")    #create new Y's row with price of the day after last day contained in X
        y = y.union(new_row)

    return X, y

"""Kode ini digunakan untuk membuat dua DataFrames baru, X dan y, dengan proses penggeseran jendela pada DataFrame asli. DataFrame X menggabungkan (`Close`) yang dikumpulkan dalam jendela dengan ukuran tertentu, sedangkan DataFrame y berisi harga penutupan untuk setiap baris di DataFrame X, mewakili harga pada hari setelah hari terakhir dalam jendela X. Proses ini membantu dalam pembentukan dataset untuk pemodelan deret waktu atau analisis lainnya yang melibatkan penggunaan jendela data."""

window = 30    #window size

X_train, y_train = slide_window(scaled_train_set, window)    #slide window on train set
X_test, y_test = slide_window(scaled_test_set, window)    #slide window on test set

"""Maaf kak, untuk run windows ini masih belum bisa sampai mendekati deadline. Jadi saya mengumpulkan seadaanya.

#Merging X and y

X and y (for both train and test) need to be merged as the Pyspark regression models require them in a single dataframe
"""

def merge_X_y(X, y):
    """
    merges two dataframes column-wise
    """
    schema = StructType(X.schema.fields + y.schema.fields)
    X_y = X.rdd.zip(y.rdd).map(lambda x: x[0]+x[1])
    return spark.createDataFrame(X_y, schema)

X_y_train = merge_X_y(X_train, y_train)
X_y_test = merge_X_y(X_test, y_test)

"""#Vectorization of windows

Windows represented as lists of days need to be converted to vectors (rows) of features as Pyspark regression models require dataframes in this form
"""

from pyspark.ml.linalg import Vectors, VectorUDT
from pyspark.sql.functions import udf
from pyspark import StorageLevel

list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())    #converts list of prices to features vector

def assemble_window(X_y):
    """
    applies list_to_vector_udf to given dataframe
    """
    return X_y.select(list_to_vector_udf(X_y["Close"]).alias("features"), X_y["value"].alias("label"))

X_y_train_vec = assemble_window(X_y_train)
X_y_test_vec = assemble_window(X_y_test)

"""#Hyperparameter tuning/model selection/evaluation

In this section linear regression and gradient-boosted trees regression models are cross-validated partitioning the train set in 3 folds in order to tune their hyperparameters and find the best model.

Then the best models found are tested on unseen data (test set) and the actual and predicted prices are plotted.
"""

from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel
from pyspark.ml.evaluation import RegressionEvaluator

def cross_validate(model, param_grid, df):
    """
    Performs grid search on given model with given parameter grid, using given dataframe as train/validation data.
    Returns the validated model (ready to be used for predictions using best parameters found)
    """
    evaluator = RegressionEvaluator(metricName="rmse")
    cv = CrossValidator(estimator=model, estimatorParamMaps=param_grid, evaluator=evaluator)
    validated_model = cv.fit(df)
    return validated_model

"""#Linear Regression"""

scaled_train_set.show()
scaled_test_set.show()

from pyspark.ml.regression import LinearRegression
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator
from pyspark.ml.evaluation import RegressionEvaluator

# Import modul lain yang diperlukan
from pyspark.ml import Pipeline

evaluator = RegressionEvaluator(metricName="rmse")

lr = LinearRegression(standardization=False)    #hindari standarisasi karena estimator tanh sudah diterapkan
param_grid = ParamGridBuilder().addGrid(lr.regParam, [0.33, 0.66]).addGrid(lr.elasticNetParam, [0.33, 0.5, 0.66]).build()    #parameter yang akan disesuaikan
pipeline = Pipeline(stages=[lr])

crossval = CrossValidator(estimator=pipeline,
                          estimatorParamMaps=param_grid,
                          evaluator=evaluator,
                          numFolds=5)  # Use 5 folds

# Training model
validated_lr_model = crossval.fit(X_y_train_vec)

# Parameter terbaik yang ditemukan
best_lr_model = validated_lr_model.bestModel.stages[0]
elasticNet = best_lr_model.getElasticNetParam()
reg = best_lr_model.getRegParam()

print("ElasicNetParam model terbaik -> ", elasticNet)
print("RegParam model terbaik -> = ", reg)

# Prediksi pada data yang belum terlihat
predictions = validated_lr_model.transform(X_y_test_vec)
RMSE = evaluator.evaluate(predictions)    # mengevaluasi prediksi menggunakan ROOT MEAN SQUARED ERROR
print("RMSE model terbaik pada data yang belum terlihat -> ", RMSE)

def plot_predictions(predictions):
    """
    plots two lines representing predicted and actual prices
    """
    pandas_df = predictions.select('label', 'prediction').toPandas()
    plt.figure(figsize=(20, 7))
    plt.plot(range(len(pandas_df['label'].values)), pandas_df['label'].values, label = 'Actual Price', color = 'blue')
    plt.plot(range(len(pandas_df['prediction'].values)), pandas_df['prediction'].values, label = 'Predicted Price', color = 'red')
    plt.xticks(np.arange(100, pandas_df.shape[0], 200))
    plt.xlabel('Time')
    plt.ylabel('Price (scaled)')
    plt.legend()
    plt.show()

plot_predictions(predictions)    #plot linear regression's predictions

"""#Gradient-boosted trees"""

from pyspark.ml.regression import GBTRegressor

gbt_r = GBTRegressor()
param_grid = ParamGridBuilder().addGrid(gbt_r.maxDepth, [4, 8, 12]).addGrid(gbt_r.featureSubsetStrategy, ['0.33', '0.66']).build()    #parameters to be tuned
validated_gbt_model = cross_validate(gbt_r, param_grid, X_y_train_vec)

#best parameters found
max_depth = validated_gbt_model.bestModel.getMaxDepth()
subsample = validated_gbt_model.bestModel.getFeatureSubsetStrategy()

print("maxDepth of best model -> ", max_depth)
print("featureSubsetStrategy of best model -> = ", subsample)

predictions = validated_gbt_model.transform(X_y_test_vec)    #test on unseen data
RMSE = evaluator.evaluate(predictions)    #evaluate predictions using ROOT MEAN SQUARED ERROR
print("RMSE of best model on unseen data ->  ", RMSE)

plot_predictions(predictions)    #plot gradient-boosted trees' predicitons